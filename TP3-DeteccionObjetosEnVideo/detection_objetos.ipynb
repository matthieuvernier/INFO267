{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 (default, Nov 12 2018, 13:43:14) \n",
      "[GCC 5.4.0 20160609]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performing object detection:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n",
      "/usr/local/lib/python3.5/dist-packages/skimage/transform/_warps.py:110: UserWarning: Anti-aliasing will be enabled by default in skimage 0.15 to avoid aliasing artifacts when down-sampling images.\n",
      "  warn(\"Anti-aliasing will be enabled by default in skimage 0.15 to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 507, 85])\n",
      "torch.Size([1, 2028, 85])\n",
      "torch.Size([1, 8112, 85])\n",
      "\t+ Batch 0, Inference Time: 0:00:02.800394\n",
      "\n",
      "Saving images:\n",
      "(0) Image: 'data/samples/IMG_3563(1).jpg'\n",
      "\t+ Label: laptop, Conf: 0.99969\n",
      "\t+ Label: person, Conf: 0.99998\n",
      "\t+ Label: person, Conf: 0.99989\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from models import Darknet\n",
    "from utils.utils import *\n",
    "\n",
    "from utils.datasets import *\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.ticker import NullLocator\n",
    "\n",
    "cuda = False\n",
    "\n",
    "os.makedirs('output', exist_ok=True)\n",
    "\n",
    "# Set up model\n",
    "model = Darknet('config/yolov3.cfg', img_size=416)\n",
    "model.load_weights('weights/yolov3.weights')\n",
    "\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "model.eval() # Set in evaluation mode\n",
    "\n",
    "\n",
    "dataloader = DataLoader(ImageFolder('data/samples', img_size=416),\n",
    "                        batch_size=1, shuffle=False, num_workers=4)\n",
    "\n",
    "classes = load_classes('data/coco.names') # Extracts class labels from file\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "\n",
    "imgs = []           # Stores image paths\n",
    "img_detections = [] # Stores detections for each image index\n",
    "\n",
    "print ('\\nPerforming object detection:')\n",
    "prev_time = time.time()\n",
    "for batch_i, (img_paths, input_imgs) in enumerate(dataloader):\n",
    "    # Configure input\n",
    "    input_imgs = Variable(input_imgs.type(Tensor))\n",
    "    \n",
    "    # Get detections\n",
    "    with torch.no_grad():\n",
    "        detections = model(input_imgs)\n",
    "        detections = non_max_suppression(detections, 80, 0.8, 0.4)\n",
    "\n",
    "\n",
    "    # Log progress\n",
    "    current_time = time.time()\n",
    "    inference_time = datetime.timedelta(seconds=current_time - prev_time)\n",
    "    prev_time = current_time\n",
    "    print ('\\t+ Batch %d, Inference Time: %s' % (batch_i, inference_time))\n",
    "\n",
    "    # Save image and detections\n",
    "    imgs.extend(img_paths)\n",
    "    img_detections.extend(detections)\n",
    "\n",
    "# Bounding-box colors\n",
    "cmap = plt.get_cmap('tab20b')\n",
    "colors = [cmap(i) for i in np.linspace(0, 1, 20)]\n",
    "\n",
    "print ('\\nSaving images:')\n",
    "# Iterate through images and save plot of detections\n",
    "for img_i, (path, detections) in enumerate(zip(imgs, img_detections)):\n",
    "\n",
    "    print (\"(%d) Image: '%s'\" % (img_i, path))\n",
    "\n",
    "    # Create plot\n",
    "    img = np.array(Image.open(path))\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(img.shape[0] - img.shape[1], 0) * (416 / max(img.shape))\n",
    "    pad_y = max(img.shape[1] - img.shape[0], 0) * (416 / max(img.shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = 416 - pad_y\n",
    "    unpad_w = 416 - pad_x\n",
    "\n",
    "    # Draw bounding boxes and labels of detections\n",
    "    if detections is not None:\n",
    "        unique_labels = detections[:, -1].cpu().unique()\n",
    "        n_cls_preds = len(unique_labels)\n",
    "        bbox_colors = random.sample(colors, n_cls_preds)\n",
    "        for x1, y1, x2, y2, conf, cls_conf, cls_pred in detections:\n",
    "\n",
    "            print ('\\t+ Label: %s, Conf: %.5f' % (classes[int(cls_pred)], cls_conf.item()))\n",
    "\n",
    "            # Rescale coordinates to original dimensions\n",
    "            box_h = ((y2 - y1) / unpad_h) * img.shape[0]\n",
    "            box_w = ((x2 - x1) / unpad_w) * img.shape[1]\n",
    "            y1 = ((y1 - pad_y // 2) / unpad_h) * img.shape[0]\n",
    "            x1 = ((x1 - pad_x // 2) / unpad_w) * img.shape[1]\n",
    "\n",
    "            color = bbox_colors[int(np.where(unique_labels == int(cls_pred))[0])]\n",
    "            # Create a Rectangle patch\n",
    "            bbox = patches.Rectangle((x1, y1), box_w, box_h, linewidth=2,\n",
    "                                    edgecolor=color,\n",
    "                                    facecolor='none')\n",
    "            # Add the bbox to the plot\n",
    "            ax.add_patch(bbox)\n",
    "            # Add label\n",
    "            plt.text(x1, y1, s=classes[int(cls_pred)], color='white', verticalalignment='top',\n",
    "                    bbox={'color': color, 'pad': 0})\n",
    "\n",
    "    # Save generated image with detections\n",
    "    plt.axis('off')\n",
    "    plt.gca().xaxis.set_major_locator(NullLocator())\n",
    "    plt.gca().yaxis.set_major_locator(NullLocator())\n",
    "    plt.savefig('output/%d.png' % (img_i), bbox_inches='tight', pad_inches=0.0)\n",
    "    plt.close()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_imgs.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
